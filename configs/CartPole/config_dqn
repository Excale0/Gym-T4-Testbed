{
  "_comment": "DQN (and, to an extend, DoubleDQN) is more stable and learns faster when updating target network after every CartPole episode",
  "algorithm": "DQN",
  "environment": "CartPole-v1",
  "episodes": 1500,

  "batch_size": 64,

  "memory_size": 50000,
  "initial_exploration_steps": 10000,

  "target_update_frequency": 1000,
  "network_train_frequency": 4,

  "epsilon": 1.0,
  "epsilon_min": 0.01,
  "epsilon_explore": 10000,

  "learning_rate": 0.001,
  "gamma": 0.99,

  "save_model": true,
  "load_model": false,

  "save_gif": true,
  "save_plot": true,
  "save_tensorboard_summary": true,

  "model_save_frequency": 1000,
  "gif_save_frequency": 500,
  "model_load_path": "/Gym-T4-Testbed/output/DQN/models/CartPole-v1_model_episode0_2019-04-11 13:28:51.990779.h5"

}